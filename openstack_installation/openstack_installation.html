<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<title>
OpenStack Grizzly Installation : Single Node
</title>
<link href='./stylesheets/bootstrap_1.css' rel='stylesheet' type='text/css'>
<meta content='nanoc 3.6.3' name='generator'>
</head>
<body>
<div class='container-fluid'>
<div class='row-fluid'>
<div class='span13' id='main'>

<h1 id="openstack-grizzly-installation---single-node">OpenStack Grizzly Installation - Single Node</h1>

<a name="introduction"><h2>Introduction</h2></a>
<p>OpenStack has come a long way from a very basic cloud orchestration platform to a distributed
set of services which are tied together and work together using a messaging layer. This distributed
nature of architecture also makes it hard to install and configure OpenStack.</p>

<p> Aim of this book is to explain in detail steps to be followed to install all the basic components
of <code>OpenStack, Grizzly</code></p>

<a name="requirements"><h2>Requirements</h2></a>
<p>In this installation you will focus on Grizzly release installation on a Single Node.
Please make sure that there are two NICs available on the Machine and assigned Appropriate IPs.</p>

<p>For this installation following mapping will be used</p>

<pre>
eth0 : 192.168.100.51
eth1 : 10.42.0.51
</pre>

<a name="getting-started"><h2 id="ubuntu-config">Ubuntu and Network Configuration</h2></a>
In this section you will perform the following tasks
<ol>
<li><a href="openstack_installation.html#conf-ubuntu">Configure Ubuntu</a></li>
<li><a href="openstack_installation.html#network-config">Network Configuration</a></li>
<li><a href="openstack_installation.html#network-util">Network Utilities Installation and Configuration</a></li>
</ol>
<h3 id="conf-ubuntu">Configure Ubuntu</h3>

<ol>


<li>
<p>Change to super user mode for rest of the document</p>

<pre class="terminal">
$sudo su
</pre>

</li>
<li>
<p>Add Grizzly repositories to get the packages for OpenStack Grizzly release:</p>

<pre class="terminal">
#apt-get install ubuntu-cloud-keyring python-software-properties software-properties-common python-keyring
#echo deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/grizzly main &gt;&gt; /etc/apt/sources.list.d/grizzly.list
</pre>

</li>
<li>Update and Upgrade the Ubuntu System:
<pre class="terminal">
#apt-get update
#apt-get upgrade
#apt-get dist-upgrade
</pre>

</li>
</ol>
<p><em>Note: On AMD machines create a volume-group called “cinder-volumes” while installing
 Ubuntu12.04 and for Intel machines create an empty partition which can later be used for 
 creating a volume-group</em></p>

<h3 id="network-config">Network Configuration</h3>
<h4>Network Interfaces</h4>
<ol>
<li><p>For OpenStack Single-Node setup you will require 2 NIC's, One NIC <code>10.42.0.51
</code> is used for external network connection i.e, Internet access and the other 
NIC <code>192.168.100.51</code> is used for internal networking (OpenStack management).</p></li>

<p><em>Note: The external NIC should have a static IP address.</em></p>

<li>
<p>Edit Network Settings to add configuration for two interfaces <code>eth0</code> and <code>eth1</code>. </p>

<div>
<img src="./images/network-interfaces.png" alt="network-interfaces">
</div>
<div>
<pre class="terminal">
#vi /etc/network/interfaces
#For Exposing OpenStack API over the internet
    auto eth1
    iface eth1 inet static
    address 10.42.0.51
    netmask 255.255.255.0
    gateway 10.42.0.1
    dns-nameservers 4.4.4.4

    #Not internet connected(used for OpenStack management)
    auto eth0
    iface eth0 inet static
    address 192.168.100.51
    netmask 255.255.255.0
</pre>

</div>


</li>

<li>
<p>Restart Network Services</p>

<pre class="terminal">
#/etc/init.d/networking restart
</pre>
</li>
</ol>
<h3 id="network-util">Network Utilities Installation and Configuration</h3>
<h4>Install VLAN and bridge-utility</h4> 
<p>VLAN and bridge-utility packages are required for OpenStack Networking</p>
<pre class="terminal">
#apt-get install -y vlan bridge-utils
</pre>

<h4>Enable IP Forwarding</h4>

<p>This step is required for GRE tunneling</p>

<pre class="terminal">
#sed -i 's/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/' /etc/sysctl.conf
</pre>

<p>To save you from rebooting, perform the following</p>

<pre class="terminal">
#sysctl net.ipv4.ip_forward=1
</pre>
<h3 id="mysql--rabbitmq">MySQL &amp; RabbitMQ Installation</h3>
<p>MySQL Database is required to store  information about OpenStack components - 
Users, Tenants, Images, Networks, Routers, Virtual Machine etc in this 
database.</p>
<p>Follow the Steps listed below</p>
<ol>
<li><a href="openstack_installation.html#install-mysql">Install MySQL</a></li>
<li><a href="openstack_installation.html#configure-mysql">Configure MySQL</a></li>
<li><a href="openstack_installation.html#install-rabbitmq">Install RabbitMQ</a></li>
<li><a href="openstack_installation.html#install-ntp">Install NTP</a></li>
</ol>

<h4 id="install-mysql">Install MySQL</h4>
<p>Execute apt-get command to download <code>mysql-server</code> and <code>python-mysql
</code> 
packages.</p>
<pre class="terminal">
#apt-get install -y mysql-server python-mysqldb
</pre>

<p>During the install, you'll be prompted for the mysql root password. Enter a password of
 your choice and verify it.</p>


<h4 id="configure-mysql">Configure MySQL</h4>
<p>Bind mysql to all IP addresses to accept incoming requests:</p>
<pre class="terminal">
#sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf
#service mysql restart
</pre>

<h4 id"install-rabbitmq">Install RabbitMQ</h4>
<p>Messaging server is the heart of the system. All the commands are first sent to the 
Messaging Service. OpenStack components picks up these messages and processes the commands
</p>

<p>The OpenStack Cloud Controller communicates with other nova components such as 
<code>Scheduler</code>, <code>Network Controller</code> and <code>Volume Controller</code>
using <code>AMQP</code> (Advanced Message Queue Protocol). Nova components use Remote 
Procedure Calls (RPC) to communicate to one another.</p>

<pre class="terminal">
#apt-get install -y rabbitmq-server
</pre>

<h4 id="install-ntp">Install NTP Service</h4>

<p>To keep all the services in sync, you need to install NTP, and if you do a multi-node 
configuration you will configure one server to be the reference server.</p>
<p>Execute apt-get to install <code>ntp</code> package</p>
<pre class="terminal">
#apt-get install -y ntp
</pre>

<a name="keystone"><h2 id="keystone">Keystone Installation</h2></a>
<p>Keystone provides Identity, Token, Catalog and Policy services as REST Endpoints which
are consumed by rest of the components</p>

<ul>
<li>Identity service : Validates auth credentials about Users, Tenants and Roles </li>
<li>Token Service : Manages and validates Tokens</li>
<li>Catalog Service : Endpoint registry for other OpenStack services</li>
<li>Policy Service : Rule Based authorization engine</li>
</ul>
<p>Keystone will be installed and then configured to work with the MySQL Databas</p>
<ol>
<li><a href="openstack_installation.html#install-keystone">Install and Verify keystone</a></li>
<li><a href="openstack_installation.html#keystone-db">Create and configure Keystone Database</a></li>
<li><a href="openstack_installation.html#environ-keystone">Configure Keystone Environment Variables</a></li>
</ol>
<h3 id="install-keystone">Install and Verify keystone</h3>
<ol>
<li>
<p>Install keystone from packages using the apt-get command.</p>

<pre class="terminal">
#apt-get install keystone
</pre>

</li>

<li>
<p>Check if keystone is running</p>

<pre class="terminal">
#service keystone status
</pre>

</li>
</ol>
<h3 id="keystone-db">Create and configure Keystone Database</h3>
<ol>
<li>
<p>Create a new MySQL database for keystone</p>

<pre class="terminal">
    mysql -u root -p
    CREATE DATABASE keystone;
    GRANT ALL ON keystone.* TO 'keystone-user'@'%' IDENTIFIED BY 'keystone-pass';
    quit;
</pre>
</li>
<li>
<p>Adapt the connection attribute in the <code>/etc/keystone/keystone.conf</code> to the new database:</p>
<pre class="terminal">
    connection = mysql://keystone-user:keystone-pass@192.168.100.51/keystone
</pre>
</li>
<li>
<p>Restart the identity service then synchronize the database</p>

<pre class="terminal">
#service keystone restart
#keystone-manage db_sync
</pre>

</li>

<li>
<p>Get the following scripts to create entries in the Keystone Database</p>

<p>Modify the HOST_IP and HOST_IP_EXT variables before executing the scripts <code>HOST_IP(192.168.100.51)</code> and <code>HOST_IP_EXT(10.42.0.51)</code></p>

<pre class="terminal">
#wget https://raw.github.com/mseknibilel/OpenStack-Grizzly-Install-Guide/OVS_SingleNode/KeystoneScripts/keystone_basic.sh
#wget https://raw.github.com/mseknibilel/OpenStack-Grizzly-Install-Guide/OVS_SingleNode/KeystoneScripts/keystone_endpoints_basic.sh
#chmod +x keystone_basic.sh
#chmod +x keystone_endpoints_basic.sh
#./keystone_basic.sh
#./keystone_endpoints_basic.sh
</pre>
</li>
</ol>
<h3 id="environ-keystone">Configure Keystone Environment Variables</h3>
<ol>
<li>
<p>Create a simple script with credentials set and source the same</p>

<pre class="terminal">
#nano creds
#Paste the following:
    export OS_TENANT_NAME=admin
    export OS_USERNAME=admin
    export OS_PASSWORD=admin_pass
    export OS_AUTH_URL="http://10.42.0.51:5000/v2.0/"
</pre>

</li>

<li>
<p>Source the script</p>

<pre class="terminal">
#source creds
</pre>

</li>

<li>
<p>Use simple CLI command to test Keystone</p>

<pre class="terminal">
#keystone user-list
</pre>

</li>
</ol>
<a name=""><h2 id="glance">Glance Installation</h2></a>

<p>OpenStack Image service provides users the ability to discover, register, and retrieve 
virtual machine images. It is also known as glance project, the Image service offers a 
REST API that allows querying of virtual machine image metadata as well as retrieval of the actual image.</p>
 
<p>Virtual machine images made available through the Image service can be stored in a variety 
of locations from simple filesystems to object-storage systems like the OpenStack Object Storage service.</p>
<p>Steps to be followed for Glance Installation</p>
<ol>
<li><a href="openstack_installation.html#glance-packages">Install Glance Packages</a></li>
<li><a href="openstack_installation.html#glance-db">Configure Glance Database</a></li>
<li><a href="openstack_installation.html#glance-config-files">Update Glance Configuration Files</a></li>
<li><a href="openstack_installation.html#glance-restart-pop-db">Restart Services and Configure Glance Database</a></li>
<li><a href="openstack_installation.html#glance-create-image">Create a Glance Image using Command Line</a></li>
</ol>

<h3 id="glance-packages">Install Glance Packages</h3>
<ol>
<li>
<p>Execute <code>apt-get</code> command to get the <code>glance</code> packages from the 
repositories</p> 
<pre class="terminal">
#apt-get install glance
</pre>

</li>
<li>
<p>Verify that glance services are running by executing the following commands</p>

<pre class="terminal">
#service glance-api status
#service glance-registry status
</pre>

</li>
</ol>
<h3 id="glance-db">Configure Glance Database</h3>
<ol>
<li>
<p>Create a new MySQL database for Glance by executing the <code>CREATE DATABASE</code> command
on the <code>mysql</code> command prompt. After this <b>GRANT ALL</b> permission for 
glance tables to <b>glance-user</b></p>

<pre class="terminal">
    mysql -u root -p
    CREATE DATABASE glance;
    GRANT ALL ON glance.* TO 'glance-user'@'%' IDENTIFIED BY 'glance-pass';
    quit;
</pre>
</li>
</ol>
<h3 id="glance-config-files">Update Glance Configuration Files</h3>
<ol>
<li>
<p>Update <code>[filter:authtoken]</code> section in 
<code>/etc/glance/glance-api-paste.ini</code></p>

<p><b>Original Code Listing:</b></p>
<pre>
[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
delay_auth_decision = true
</pre>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    [filter:authtoken]
    paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
    delay_auth_decision = true
    auth_host = 192.168.100.51
    auth_port = 35357
    auth_protocol = http
    admin_tenant_name = service
    admin_user = glance
    admin_password = service_pass
</pre>
</li>

<li>
<p>Update <code>[filter:authtoken]</code> section in 
<code>/etc/glance/glance-registry-paste.ini</code></p>
<p><b>Original Code Listing:</b></p>
<pre>
[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
</pre>
<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    [filter:authtoken]
    paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
    auth_host = 192.168.100.51
    auth_port = 35357
    auth_protocol = http
    admin_tenant_name = service
    admin_user = glance
    admin_password = service_pass
</pre>

</li>

<li>
<p>Update <code>/etc/glance/glance-api.conf</code> as shown below</p>
<ol>
<li>
<p>sql_connection value is configured to point to MySQL Instance configured above</p>
<p><b>Original Code Listing:</b></p>
<pre>
sql_connection = sqlite:///glance.sqlite
</pre>
<p><b>Modified Code Listing:</b></p>
<pre>
sql_connection = mysql://glance-user:glance-pass@192.168.100.51/glance
</pre>
</li>
<li>
<p>Paste Flavor in the file <code>glance-api.conf</code> is configured to keystone</p>
<p><b>Original Code Listing:</b></p>
<pre>
#flavor=
</pre>
<p><b>Modified Code Listing:</b></p>
<pre>
flavor = keystone
</pre>
<i>Note : The Python Paste package contains Python modules that help in 
implementing WSGI middleware. Keystone is configured to be part of the paste pipeline
for Glance APIs
</i>
</li>
</ol>

<li>
<p>Update the <code>/etc/glance/glance-registry.conf</code> with:</p>

<pre class="terminal">
    sql_connection = mysql://glance-user:glance-pass@192.168.100.51/glance
    And:
    [paste_deploy]
    flavor = keystone
</pre>
</li>

<h3 id="glance-restart-pop-db">Restart Services and Configure Glance Database</h3>
<ol>
<li>
<p>Restart the glance-api and glance-registry services:</p>

<pre class="terminal">
#service glance-api restart; service glance-registry restart
</pre>

</li>

<li>
<p>Synchronize the glance database using <code>glance-manage</code> command</p>

<pre class="terminal">
#glance-manage db_sync
</pre>

</li>

<li>
<p>Restart the services again to take into account the new modifications</p>

<pre class="terminal">
#service glance-registry restart; service glance-api restart
</pre>

</li>
</ol>
<h3 id="glance-create-image">Create a Glance Image using Command Line</h3>
<ol>
<li>
<p>To verify the image service Glance, upload the cirros cloud image directly from the internet:</p>

<pre class="terminal">
#glance image-create --name myFirstImage --is-public true --container-format bare --disk-format qcow2 --location https://launchpad.net/    cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img

+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | d972013792949d0d3ba628fbe8685bce     |
| container_format | bare                                 |
| created_at       | 2013-10-08T18:59:18                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | acafc7c0-40aa-4026-9673-b879898e1fc2 |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | CirrOS 0.3.1                         |
| owner            | efa984b0a914450e9a47788ad330699d     |
| protected        | False                                |
| size             | 13147648                             |
| status           | active                               |
| updated_at       | 2013-05-08T18:59:18                  |
+------------------+--------------------------------------+
</pre>

</li>

<li>
<p>List the uploaded image</p>

<pre class="terminal">
#glance image-list
+--------------------------------------+-----------------+-------------+------------------+----------+--------+
| ID                                   | Name            | Disk Format | Container Format | Size     | Status |
+--------------------------------------+-----------------+-------------+------------------+----------+--------+
| acafc7c0-40aa-4026-9673-b879898e1fc2 | CirrOS 0.3.1    | qcow2       | bare             | 13147648 | active |
+--------------------------------------+-----------------+-------------+------------------+----------+--------+
</pre>

</li>
</ol>
<a name=""><h2 id="quantum">Quantum Installation</h2></a>
Quantum manages software-defined networking for your OpenStack installation. Quantum has the following object abstractions: networks, subnets, and routers. Each has functionality that mimics its physical counterpart: networks contain subnets, and routers route traffic between different subnet and networks. Quantum should have at least one external network. The external network represents the view into a slice of the external network that is accessible outside the OpenStack installation. In addition to external networks, any Neutron set up has one or more internal networks. These software-defined networks connect directly to the VMs. Only the VMs on any given internal network, or those on subnets connected through interfaces to a similar router, can access VMs connected to the same network directly. </p>

<p>Follow the Steps listed below</p>
<ol>
<li><a href="openstack_installation.html#open-vswitch">Install and Configure Open vSwitch</a></li>
<li><a href="openstack_installation.html#quantum_components">Install Quantum components</a></li>
<li><a href="openstack_installation.html#create-db">Create Quantum Database</a></li>
<li><a href="openstack_installation.html#verify-services">Verify Quantum Services</a></li>
<li><a href="openstack_installation.html#update-config-files">Update Quantum Configuration Files</a></li>
<li><a href="openstack_installation.html#restart-services">Restart Quantum Services</a></li>
</ol>


<h3 id="open-vswitch">Install and Configure Open vSwitch</h3>
<ol>
<li>
<p>Install Open vSwitch Packages:</p>

<pre class="terminal">
#apt-get install -y openvswitch-switch openvswitch-datapath-dkms
</pre>

</li>

<li>
<p>Create the bridges:</p>
<p><code>br-int</code> will be used for VM integration</p>
<pre class="terminal">
#br-int will be used for VM integration
#ovs-vsctl add-br br-int
</pre>


<p><code>br-ex</code> is used to access the external network.</p>

<pre class="terminal">
#ovs-vsctl add-br br-ex
</pre>

</li>

<li>
<p>This will guide you to setting up the <code>br-ex</code> interface. Edit eth1 in <code>/etc/network/interfaces</code>:</p>

<pre class="terminal">
# VM internet Access
auto eth1
iface eth1 inet manual
up ifconfig $IFACE 0.0.0.0 up
up ip link set $IFACE promisc on
down ip link set $IFACE promisc off
down ifconfig $IFACE down
</pre>

</li>

<li>
<p>Add <code>eth1</code> interface to <code>br-ex</code></p>

<p>Internet connectivity will be lost after this step but this won't affect OpenStack's work</p>

<pre class="terminal">
#ovs-vsctl add-port br-ex eth1
</pre>

</li>

<li>
<p>Optional, If you want to get internet connection back, you can assign the <code>eth1's</code> IP address to the <code>br-ex</code> in the <code>/etc/network/interfaces</code> file</p>
<pre class="terminal">
auto br-ex
iface br-ex inet static
address 10.42.0.51
netmask 255.255.255.0
gateway 10.42.0.1
dns-nameservers 8.8.8.8
</pre>
</li>
</ol>

<h3 id="quantum_components">Install Quantum components</h3>
<p>Install Quantum Components</p>
<pre class="terminal">
#apt-get install -y quantum-server quantum-plugin-openvswitch quantum-plugin-openvswitch-agent dnsmasq quantum-dhcp-agent quantum-l3-agent
</pre>

<h3 id="create-db">Create Quantum Database</h3>
<pre class="terminal">
mysql -u root -p
CREATE DATABASE quantum;
GRANT ALL ON quantum.* TO 'quantum-user'@'%' IDENTIFIED BY 'quantum-pass';
quit;
</pre>

<h3 id="verify-services">Verify Quantum Services</h3>
<pre class="terminal">
#cd /etc/init.d/; for i in $( ls quantum-* ); do sudo service $i status; done
</pre>

<h3 id="update-config-files">Update Quantum Configuration Files</h3>
<ol><li>
<p>Configure <code>/etc/quantum/api-paste.ini</code></p>
<pre class="terminal">
[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
auth_host = 192.168.100.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = quantum
admin_password = service_pass
</pre>
</li>

<li>
<p>Configure Quantum Plugin</p>
<p>Edit the OVS plugin configuration file <code>/etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini with</code>:</p>
<p>Under the database section</p>
<pre class="terminal">
[DATABASE]
sql_connection = mysql://quantum-user:quantum-pass@192.168.100.51/quantum
</pre>

<p>Under the OVS section</p>
<pre class="terminal">
[OVS]
tenant_network_type = gre
tunnel_id_ranges = 1:1000
integration_bridge = br-int
tunnel_bridge = br-tun
local_ip = 192.168.100.51
enable_tunneling = True

#Firewall driver for realizing quantum security group function
[SECURITYGROUP]
firewall_driver = quantum.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
</pre>
</li>

<li>
<p>Update  <code>/etc/quantum/metadata_agent.ini</code></p>

<pre class="terminal">
#The Quantum user information for accessing the Quantum API.
auth_url = http://192.168.100.51:35357/v2.0
auth_region = RegionOne
admin_tenant_name = service
admin_user = quantum
admin_password = service_pass

#IP address used by Nova metadata server
nova_metadata_ip = 127.0.0.1

#TCP Port used by Nova metadata server
nova_metadata_port = 8775
metadata_proxy_shared_secret = helloOpenStack
</pre>

</li>
<li>
<p>Edit your <code>/etc/quantum/quantum.conf</code></p>

<pre class="terminal">
[keystone_authtoken]
auth_host = 192.168.100.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = quantum
admin_password = service_pass
signing_dir = /var/lib/quantum/keystone-signing
</pre>
</li>
</ol>

<h3 id="restart-services">Restart Quantum Services</h3>
<pre class="terminal">
#cd /etc/init.d/; for i in $( ls quantum-* ); do sudo service $i restart; done
#service dnsmasq restart
</pre>

<a name=""><h2 id="nova">Nova Installation</h2></a>

<p>OpenStack Compute (Nova) is a cloud computing fabric controller (the main part of an IaaS system). Nova's architecture is designed to scale horizontally on standard hardware with no proprietary hardware or software requirements and provide the ability to integrate with legacy systems and third party technologies. It is designed to manage and automate pools of computer resources and can work with widely available virtualization technologies, as well as bare metal and high-performance computing (HPC) configurations.</p>

<h3 id="kvm">KVM Installation</h3>

<p>Kernel-based Virtual Machine (KVM) is a virtualization infrastructure for the Linux kernel which turns it into a hypervisor. KVM requires a processor with hardware virtualization extension.</p>

<p>For production environments the most tested hypervisors are KVM and Xen-based hypervisors. KVM runs through libvirt, Xen runs best through XenAPI calls. KVM is selected by default and requires the least additional configuration.</p>


<p>Steps to be followed for KVM Installation</p>
<ol>
<li><a href="openstack_installation.html#pre-requisites">Pre-requisites for KVM Installation</a></li>
<li><a href="openstack_installation.html#install-and-configure-kvm">Install and Configure KVM</a></li>
</ol>

<h4 id="pre-requisites">Pre-requisites for KVM Installation</h4>
<ol>
<li>
<p>Checking for hardware virtualization support:</p>

<p>The processors of your compute host need to support virtualization technology (VT) to use KVM.</p>
<p>Install the cpu package and use the kvm-ok command to check if your processor has VT support.</p>

<pre class="terminal">
#apt-get install cpu-checker
#kvm-ok
</pre>

If VT is enabled, you will see output similar to the listing below

<pre class="terminal">
INFO: /dev/kvm exists
KVM acceleration can be used
</pre>

</li>
</ol>

<h4 id="install-and-configure-kvm">Install and Configure KVM</h4>
<ol>
<li>

<pre class="terminal">
#apt-get install kvm libvirt-bin pm-utils
</pre>

</li>

<li>
<p>Edit the <code>cgroup_device_acl</code> array in the <code>/etc/libvirt/qemu.conf</code> file to:</p>


<p><b>Original Code Listing:</b></p>
<pre class="terminal">
    #cgroup_device_acl = [
    #"/dev/null", "/dev/full", "/dev/zero",
    #"/dev/random", "/dev/urandom",
    #"/dev/ptmx", "/dev/kvm", "/dev/kqemu",
    #"/dev/rtc", "/dev/hpet","/dev/net/tun"
    #]
</pre>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    cgroup_device_acl = [
    "/dev/null", "/dev/full", "/dev/zero",
    "/dev/random", "/dev/urandom",
    "/dev/ptmx", "/dev/kvm", "/dev/kqemu",
    "/dev/rtc", "/dev/hpet","/dev/net/tun"
    ]
</pre>

</li>

<li>
<p>Delete default virtual bridge to avoid any confusion</p>

<pre class="terminal">
#virsh net-destroy default
#virsh net-undefine default
</pre>

</li>

<li>
<p>Enable live migration by updating <code>/etc/libvirt/libvirtd.conf</code> file:</p>

<p><b>Original Code Listing:</b></p>
<pre class="terminal">
    #listen_tls = 0
    .
    .
    #listen_tcp = 1
    .
    .
    #auth_tcp = "none"
</pre>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    listen_tls = 0
    .
    .
    listen_tcp = 1
    .
    .
    auth_tcp = "none"
</pre>

<p>Edit <code>libvirtd_opts</code> variable in <code>/etc/init/libvirt-bin.conf</code> file:</p>

<p><b>Original Code Listing:</b></p>
<pre class="terminal">
    env libvirtd_opts="-d"
</pre>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    env libvirtd_opts="-d -l"
</pre>


<p>Edit <code>/etc/default/libvirt-bin</code> file</p>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    libvirtd_opts="-d -l"
</pre>

<p><b>Modified Code Listing:</b></p>
<pre class="terminal">
    libvirtd_opts="-d -l"
</pre>

</li>

<li>
<p>Restart the <code>libvirt</code> service and <code>dbus</code> to load the new values</p>

<pre class="terminal">
#service dbus restart &amp;&amp; service libvirt-bin restart
</pre>


</li>
</ol>

<h3 id="Pre-requsites">Nova Installation</h3>

<p>Steps to be followed for Nova Installation</p>
<ol>
<li><a href="openstack_installation.html#nova-packages">Install Nova Packages</a></li>
<li><a href="openstack_installation.html#nova-db">Configure Nova Database</a></li>
<li><a href="openstack_installation.html#configure-nova-files">Update Nova Configuration Files</a></li>
<li><a href="openstack_installation.html#restart-dbsync-nova">Configure Nova Database and Restart Services</a></li>
</ol>

<h4 id="nova-packages">Install Nova Packages</h4>

<pre class="terminal">
#apt-get install  nova-api nova-cert novnc nova-consoleauth nova-scheduler nova-novncproxy nova-doc nova-conductor nova-compute-kvm
</pre>

<p>Check the status of all nova-services</p>

<pre class="terminal">
#cd /etc/init.d/; for i in $( ls nova-* ); do service $i status; cd; done
</pre>

<h4 id="nova-db">Configure Nova Database</h4>
<p>Create a new MySQL database for Nova by executing the <code>CREATE DATABASE</code> command
on the <code>mysql</code> command prompt. After this <b>GRANT ALL</b> permission for 
nova tables to <b>nova-user</b></p>

<pre class="terminal">
mysql -u root -p
CREATE DATABASE nova;
GRANT ALL ON nova.* TO 'nova-user'@'%' IDENTIFIED BY 'nova-pass';
quit;
</pre>

<h4 id="configure-nova-files">Update Nova Configuration Files</h4>
<ol>
<li>
<p>Update <code>[filter:authtoken]</code> section in the <code>/etc/nova/api-paste.ini</code> file to this:</p>

<pre class="terminal">
[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
auth_host = 192.168.100.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = service_pass
signing_dirname = /tmp/keystone-signing-nova
# Workaround for https://bugs.launchpad.net/nova/+bug/1154809
auth_version = v2.0
</pre>

</li>

<li>
<p>The Compute service supports a large number of configuration options. These options are specified
in a configuration file whose default location is /etc/nova/nova.conf.</p>

<p>Update <code>/etc/nova/nova.conf</code> like this:</p>
<ul>
<li>
<p>Almost all of the configuration options are in the DEFAULT section.</p>
<pre class="terminal">
[DEFAULT]
logdir=/var/log/nova (The base directory used for relative --log-file paths)
state_path=/var/lib/nova (Top-level directory for maintaining nova's state)
lock_path=/run/lock/nova (Directory to use for lock files. Default to a temp directory)
verbose=True (if FALSE, Print more verbose output (set logging level to INFO instead of default WARNING level))
api_paste_config=/etc/nova/api-paste.ini (File name for the paste.deploy config for nova-api)
compute_scheduler_driver=nova.scheduler.simple.SimpleScheduler (Default driver to use for the scheduler)
rabbit_host=192.168.100.51 (The RabbitMQ broker address where a single node is used)
nova_url=http://192.168.100.51:8774/v1.1/
sql_connection=mysql://nova-user:nova-pass@192.168.100.51/nova (The SQLAlchemy connection string
used to connect to the database)
root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf
</pre>
</li>

<li>
<p>Configure Authentication and Authorization</p>
<p>There are different methods of authentication for the OpenStack Compute project, including no
authentication. The preferred system is the OpenStack Identity Service, code-named Keystone.</p>
<pre class="terminal">
# Auth 
use_deprecated_auth=false
auth_strategy=keystone (The strategy to use for auth: noauth or keystone)
</pre>
</li>

<li>
<p>Glance interaction with Nova</p>
<pre class="terminal">
# Imaging service 
glance_api_servers=192.168.100.51:9292 (A list of the glance api servers available to nova. Prefix with
https:// for ssl-based glance api servers. ([hostname|ip]:port)) image_service=nova.image.glance.GlanceImageService
</pre>
</li>

<li>
<p>The VNC Proxy is an OpenStack component that allows users of the Compute service to access their
 instances through VNC clients.</p>
<p>To enable <code>vncproxy</code> in your cloud, in addition to running one or both of the proxies and nova-consoleauth, you need to configure the following options in <code>nova.conf</code> on your compute hosts. </p>
<pre class="terminal">
# Vnc configuration

novnc_enabled=true (Defaults to enabled. If this option is disabled your instances will launch without VNC support)
novncproxy_base_url=http://10.42.0.51:6080/vnc_auto.html (location of vnc console proxy, Modify IP address according to your external IP address)
novncproxy_port=6080 (Port that the novncproxy should bind to)
vncserver_proxyclient_address=192.168.100.51 (This is the address of the compute host that nova will instruct proxies to use when connecting to instance vncservers)
vncserver_listen=0.0.0.0 (This is the address that vncservers will bind, and should be overridden in production deployments as a private address. Applies to libvirt only.
                              For multi-host libvirt deployments this should be set to a host management IP on the same as the proxies)
</pre>
</li>

<li>
<p>Configure Nova with Quantum</p>
<p>Unlike traditional Nova deployments, when Quantum is in use, Nova should not run a nova-network.
Instead, Nova delegates almost all of the network-related decisions to Quantum. This means many of
the network-related CLI command and configuration options you are familiar with from using Nova do not work with Quantum.</p>
<pre class="terminal">
# Network settings
    
network_api_class=nova.network.quantumv2.api.API (The full class name of the network API class to use)
quantum_url=http://192.168.100.51:9696  (URL for connecting to quantum)
quantum_auth_strategy=keystone (auth strategy for connecting to quantum in admin context)
quantum_admin_tenant_name=service (tenant name for connecting to quantum in admin context)
quantum_admin_username=quantum (username for connecting to quantum in admin context)
quantum_admin_password=service_pass (auth url for connecting to quantum in admin context)
quantum_admin_auth_url=http://192.168.100.51:35357/v2.0 (auth url for connecting to quantum in admin context)
libvirt_vif_driver=nova.virt.libvirt.vif.LibvirtHybridOVSBridgeDriver (The libvirt VIF driver to configure the VIFs)
linuxnet_interface_driver=nova.network.linux_net.LinuxOVSInterfaceDriver
#If you want Quantum + Nova Security groups
firewall_driver=nova.virt.firewall.NoopFirewallDriver (Firewall driver (defaults to hypervisor specific iptables driver))
security_group_api=quantum (The full class name of the security API class)
#If you want Nova Security groups only, comment the two lines above and uncomment line -1-.
#-1-firewall_driver=nova.virt.libvirt.firewall.IptablesFirewallDriver
</pre>
</li>

<li>
<p>Description of configuration options for metadata</p>
<p>The Compute service uses a special metadata service to enable virtual machine instances to retrieve instance-specific data.
Instances access the metadata service at http://169.254.169.254. The metadata service supports 
two sets of APIs: an OpenStack metadata API and an EC2-compatible API. Each of the APIs is versioned by date.</p>
<pre class="terminal">
#Metadata 

service_quantum_metadata_proxy = True (All compute hosts share the same dhcp address.)
quantum_metadata_proxy_shared_secret = helloOpenStack 
metadata_host = 192.168.100.51 (the ip for the metadata api server)
metadata_listen = 127.0.0.1 (IP address for metadata api to listen)
metadata_listen_port = 8775 (port for metadata api to listen)
</pre>
</li>

<li>
<p>Connect to a hypervisor through <code>libvirt</code></p>
<pre class="terminal">
# Compute 

compute_driver=libvirt.LibvirtDriver (Driver to use for controlling virtualization. 
Options include: libvirt.LibvirtDriver, xenapi.XenAPIDriver, fake.FakeDriver, baremetal.BareMetalDriver, vmwareapi.VMwareESXDriver, vmwareapi.VMwareVCDriver)
</pre>
</li>

<li>
<p>The <code>volume_api_class</code> setting is the default setting for grizzly.</p>
<pre class="terminal">
# Cinder 

volume_api_class=nova.volume.cinder.API (The full class name of the volume API class to use)
osapi_volume_listen_port=5900 (port for os volume api to listen )
</pre>
</li>
</ul>
</li>

<li>
<p>Update <code>/etc/nova/nova-compute.conf</code></p>

<p>KVM is configured as the default hypervisor for Compute.</p>
<p>Description of configuration options for hypervisor
The nova.conf used by the nova-compute service should contain the following flags to ensure correct vif-plugging. 
If your integration bridge name is something other than “br-int”, change the first flag listed below:</p>
<pre class="terminal">
[DEFAULT]
libvirt_type=kvm (Libvirt domain type (valid options are: kvm, lxc, qemu, uml, xen))
libvirt_ovs_bridge=br-int (Internal Bridge)
libvirt_vif_type=ethernet 
libvirt_vif_driver=nova.virt.libvirt.vif.LibvirtHybridOVSBridgeDriver (The libvirt VIF driver to configure the VIFs)
libvirt_use_virtio_for_bridges=True (Update an existing OpenStack instance after a global flag change)
</pre>

</li>
</ol>

<h4 id="restart-dbsync-nova">Configure Nova Database and Restart Services</h4>
<ol>
<li>
<p>Synchronize the nova database using nova-manage command</p>

<pre class="terminal">
#nova-manage db sync
</pre>

</li>
    
<li>
<p>Restart nova-* services</p>

<pre class="terminal">
#cd /etc/init.d/; for i in $( ls nova-* ); do sudo service $i restart; done
</pre>

</li>

<li>
<p>Check for the smiling faces on nova-* services to confirm your installation</p>

<pre class="terminal">
#nova-manage service list
</pre>

</li>
</ol>

<a name=""><h2 id="cinder">Cinder Installation</h2></a>
<p>OpenStack Block Storage (Cinder) provides persistent block level storage devices for use with OpenStack compute instances. The block storage system manages the creation, attaching and detaching of the block devices to servers.</p>
<p>The OpenStack Block Storage service works though the interaction of a series of daemon processes named cinder-* that reside persistently on the host machine or machines. The binaries can all be run from a single node, or spread across multiple nodes. They can also be run on the same node as other OpenStack services.</p>
<p>OpenStack Block Storage provides persistent High Performance Block Storage resources that can be consumed by OpenStack Compute instances. This includes secondary attached storage similar to Amazon's Elastic Block Storage (EBS). In addition images can be written to a Block Storage device and specified for OpenStack Compute to use a bootable persistent instance.
OpenStack Block Storage requires some form of back-end storage that the service is built on. The default implementation is to use LVM on a local Volume Group named "cinder-volumes".</p>
<p>Steps to be followed for Cinder Installation</p>
<ol>
<li><a href="openstack_installation.html#cinder-packages">Install Cinder Packages</a></li>
<li><a href="openstack_installation.html#cinder-db">Create Cinder Database</a></li>
<li><a href="openstack_installation.html#configure-cinder-files">Update Cinder Configuration Files</a></li>
<li><a href="openstack_installation.html#cinder-dbsync">Configure Cinder Database</a></li>
<li><a href="openstack_installation.html#cinder-volume">Create Cinder Volume</a></li>
<li><a href="openstack_installation.html#restart-cinder">Restart Cinder Services</a></li>
</ol>

<h3 id="cinder-packages">Install Cinder Packages</h3>

<ol>
<li>
<p>Install the required packages:</p>

<pre class="terminal">
#apt-get install -y cinder-api cinder-scheduler cinder-volume iscsitarget open-iscsi iscsitarget-dkms
</pre>

</li>

<li>
<p>Configure the iscsi services:</p>

<pre class="terminal">
#sed -i 's/false/true/g' /etc/default/iscsitarget
</pre>

</li>
<li>
<p>Restart the services:</p>

<pre class="terminal">
#service iscsitarget start
#service open-iscsi start
</pre>

</li>
</ol>

<h3 id="cinder-db">Create Cinder Database</h3>
<p>Prepare a Mysql database for Cinder:</p>
<pre class="terminal">
mysql -u root -p
CREATE DATABASE cinder;
GRANT ALL ON cinder.* TO 'cinderUser'@'%' IDENTIFIED BY 'cinderPass';
quit;
</pre>

<h3 id="configure-cinder-files">Update Cinder Configuration Files</h3>
<ol>
<li>
<p>Configure <code>/etc/cinder/api-paste.ini</code> like the following:</p>

<pre class="terminal">
[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
service_protocol = http
service_host = 10.42.0.51
service_port = 5000
auth_host = 192.168.100.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = cinder
admin_password = service_pass
</pre>

</li>

<li>
<p>Edit the <code>/etc/cinder/cinder.conf</code> to:</p>

<pre class="terminal">
[DEFAULT]
rootwrap_config=/etc/cinder/rootwrap.conf
sql_connection = mysql://cinderUser:cinderPass@192.168.100.51/cinder
api_paste_config = /etc/cinder/api-paste.ini
iscsi_helper=ietadm
volume_name_template = volume-%s
volume_group = cinder-volumes
verbose = True
auth_strategy = keystone
#osapi_volume_listen_port=5900
</pre>

</li>
</ol>

<h3 id="cinder-dbsync">Configure Cinder Database</h3>

<p>Then, synchronize your database:</p>
<pre class="terminal">
#cinder-manage db sync
</pre>

<h3 id="cinder-volume">Create Cinder Volume</h3>
<p>Cinder uses volume groups for storage. There are two ways to create the volume group.</p>

<ul>
<li>Create a LVM Volume Group by using a partition</li>
<li>Create a LVM Volume Group by using a container file as loop device</li>
</ul>

<p><b>Create a Volume Group by using a partition</b></p>
<p>At the time of installation of Ubuntu, leave a raw partition. Do not format or install anything in that partition.</p>
<p>Follow below steps to use the empty partiton for creating cinder volme group. Assume <code>/dev/sda13</code> is your empty partition.</p>

<ol><li>
<p>Create Physical Volume</p>
<pre class="terminal">
#pvcreate /dev/sda13
</pre>
</li>

<li>
<p>Create Volume Group</p>
<pre class="terminal">#vgcreate cinder-volumes /dev/sda13</pre>
<p>The Volume group can also be created using the user interface <code>system-config-lvm</code> as an alternative to command line</p>
<p>Alllocate the empty partition to "cinder-volumes"</p>

<pre class="terminal">
#sudo apt-get install system-config-lvm
#system-config-lvm
#Select the partition and name it as "cinder-volumes"
</pre>
</li>
</ol>

<p><b>Creating a LVM Volume Group by using a container file as loop device</b></p>
<p>If there is no partition available, we can use loop back device to create volume group.</p>
<ol><li>
<p>Create a container file /dev/zero</p>
<pre class="terminal">
#dd if=/dev/zero of=cinder-volumes bs=1 count=0 seek=20G </pre>
</li>

<li>    
<p>Create the loop device</p>
<pre class="terminal">#losetup /dev/loop2 cinder-volume</pre>
</li>

<li><p>Create the partition</p>
<pre class="terminal">
#fdisk /dev/loop2
#Select the following options:
n
p
1
ENTER
ENTER
t
8e
w</pre>
</li>

<li><p>Create the physical volume</p>
<pre class="terminal">#pvcreate /dev/loop2</pre>
</li>

<li><p>Create the volume group</p>
<pre class="terminal">
#vgcreate cinder-volumes /dev/loop2</pre>
</li>
</ol>

<h3 id="restart-cinder">Restart Cinder Services</h3>
<ol><li>
<p>Restart the cinder services:</p>

<pre class="terminal">
#cd /etc/init.d/; for i in $( ls cinder-* ); do sudo service $i restart; done</pre>

</li>

<li>
<p>Verify if cinder services are running:</p>

<pre class="terminal">
#cd /etc/init.d/; for i in $( ls cinder-* ); do sudo service $i status; done</pre>

</li>
</ol>

<a name=""><h2 id="horizon">Horizon Installation</h2></a>

<p>OpenStack Dashboard (Horizon) provides administrators and users a graphical interface to access, provision and automate cloud-based resources.
The dashboard is just one way to interact with OpenStack resources.</p>
<ol>
<li>
<p>To install horizon, proceed like this</p>

<pre class="terminal">
#apt-get -y install openstack-dashboard memcached
</pre>

</li>

<li>
<p>Optional:If you don't like the OpenStack ubuntu theme, you can remove the package to disable it:</p>

<pre class="terminal">
#dpkg --purge openstack-dashboard-ubuntu-theme
</pre>

</li>

<li>
<p>Reload Apache and memcached:</p>

<pre class="terminal">
#service apache2 restart; service memcached restart
</pre>

<p>You can now access your OpenStack <code>10.42.0.51/horizon</code> with credentials <code>admin:admin_pass</code>.</p>
</li>
</ol>
<a name="first-vm"><h2 id="create-vm">Create a VM</h2></a>

<ol>
<li>
<p>Create a external network:</p>

<pre class="terminal">
#quantum net-create public-net --router:external=True

Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 21ea48fb-ee1e-46a4-b589-b3c2b359291d |
| name                      | public-net                           |
| provider:network_type     | gre                                  |
| provider:physical_network |                                      |
| provider:segmentation_id  | 1                                    |
| router:external           | True                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | 2b942273713741b1868eb86b11e08df8     |
+---------------------------+--------------------------------------+
</pre>

 <img src="./images/create_ext_net.png">
</li>


<li>
<p>Create a subnet:</p>

<pre class="terminal">
#quantum subnet-create --tenant-id 2b942273713741b1868eb86b11e08df8 --name public-net-subnet01 --gateway 10.42.0.1 public-net 10.42.0.0/24 --enable_dhcp False

Created a new subnet:
+------------------+----------------------------------------------+
| Field            | Value                                        |
+------------------+----------------------------------------------+
| allocation_pools | {"start": "10.42.0.2", "end": "10.42.0.254"} |
| cidr             | 10.42.0.0/24                                 |
| dns_nameservers  |                                              |
| enable_dhcp      | False                                        |
| gateway_ip       | 10.42.0.1                                    |
| host_routes      |                                              |
| id               | d5b8d223-c2b2-4b87-ae7c-187d37f8b762         |
| ip_version       | 4                                            |
| name             | public-net-subnet01                          |
| network_id       | 21ea48fb-ee1e-46a4-b589-b3c2b359291d         |
| tenant_id        | 2b942273713741b1868eb86b11e08df8             |
+------------------+----------------------------------------------+
</pre>

<img src="./images/create_subnet_ext.png">
</li>

<li>
<p>Allocation of IP's to VM's:</p>


<p>This is not tried but, we can try allocation pool also - this command not tried , pls verify before trying.</p>

<pre class="terminal">
#quantum subnet-create --tenant-id 6973efb023c748d6b8a4fff747faad92 --name public-net-subnet01 --gateway 10.42.0.1 public-net 10.42.0.0/24 --enable_dhcp False --allocation-pool start=10.42.0.75,end=10.42.0.254
</pre>

</li>

<li>
<p>Create a private-network:</p>

<pre class="terminal">
#quantum net-create private-net

Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 428dbfc9-73e1-4e8d-88e2-471a3e91f6a6 |
| name                      | private-net                          |
| provider:network_type     | gre                                  |
| provider:physical_network |                                      |
| provider:segmentation_id  | 2                                    |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | 2b942273713741b1868eb86b11e08df8     |
+---------------------------+--------------------------------------+
</pre>

<img src="./images/create_pvt_net.png">
</li>


<li>
<p>Attach subnet to private network:</p>

<pre class="terminal">
#quantum subnet-create --name private-subnet private-net 10.0.0.0/24

Created a new subnet:
+------------------+--------------------------------------------+
| Field            | Value                                      |
+------------------+--------------------------------------------+
| allocation_pools | {"start": "10.0.0.2", "end": "10.0.0.254"} |
| cidr             | 10.0.0.0/24                                |
| dns_nameservers  |                                            |
| enable_dhcp      | True                                       |
| gateway_ip       | 10.0.0.1                                   |
| host_routes      |                                            |
| id               | 5421a4eb-5b4b-4c3e-9b56-6bb721f99653       |
| ip_version       | 4                                          |
| name             | private-subnet                             |
| network_id       | 428dbfc9-73e1-4e8d-88e2-471a3e91f6a6       |
| tenant_id        | 2b942273713741b1868eb86b11e08df8           |
+------------------+--------------------------------------------+
</pre>

<img src="./images/create_subnet_pvt.png">
</li>

<li>
<p>Create router:</p>

<pre class="terminal">
#quantum router-create router1

Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| external_gateway_info |                                      |
| id                    | ca972d3b-788e-4e16-8552-dd335575c5c0 |
| name                  | router1                              |
| status                | ACTIVE                               |
| tenant_id             | 2b942273713741b1868eb86b11e08df8     |
+-----------------------+--------------------------------------+
</pre>

<img src="./images/create_router.png">
</li>

<li>
<p>Uplink router to public network:</p>

<pre class="terminal">
#quantum router-gateway-set router1 public-net
</pre>

</li>

<li>
<p>Attach private network to router:</p>

<pre class="terminal">
#quantum router-interface-add router1 private-subnet
</pre>

<img src="./images/router_add_iface.png">
</li>

<li>
<p>To show the net list:</p>

<pre class="terminal">
#ip netns list
qrouter-ca972d3b-788e-4e16-8552-dd335575c5c0
</pre>

<pre class="terminal">
#ip netns exec qrouter-ca972d3b-788e-4e16-8552-dd335575c5c0 ip addr list
10: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
11: qg-e045e129-e0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN 
    link/ether fa:16:3e:23:30:ef brd ff:ff:ff:ff:ff:ff
    inet 10.42.0.2/24 brd 10.42.0.255 scope global qg-e045e129-e0
    inet6 fe80::f816:3eff:fe23:30ef/64 scope link 
       valid_lft forever preferred_lft forever
12: qr-696c2816-7e: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN 
    link/ether fa:16:3e:c8:2f:1a brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.1/24 brd 10.0.0.255 scope global qr-696c2816-7e
    inet6 fe80::f816:3eff:fec8:2f1a/64 scope link 
       valid_lft forever preferred_lft forever
</pre>

</li>
</ol>
<ul>
<li>
<p>Create a Security Group:</p>
<img src="./images/create_security_group_desc.png">
</li>

<li>
<p>Create Keypairs:</p>
<p>
<img src="./images/create_keypair.png"></p>
</li>
<li>
<p>Add Rule:</p>
<img src="./images/add_rule.png">
</li>

<li>
<p>Edit Security Group Rules:</p>
<!--<img src="./images/security_group_rules.png">-->
</li>

<li>
<p>Access and Security</p>
<img src="./images/access_and_security.png">
</li>


<code>Note: Create a VM using the Horizon dashboard</code>


<li>
<p>Before creating a VM</p>
<img src="./images/empty_instances.png">
</li>

<li>
<p>Launch Instance:</p>
 <img src="./images/launch_instance.png">
</li>

<li>
<p>Launch Instance Keypair:</p>
 <img src="./images/launch_instance_keypair.png">
</li>

<li>
<p>Launch Instance Networking:</p>
 <img src="./images/launch_instance_networking.png">
</li>

<li>
<p>My first VM</p>
 <img src="./images/first_instance.png">
</li>

<li>
<p>Network Topology</p>
 <img src="./images/network_topology.png">
</li>

<li>
<p>To ping VM:</p>

<pre class="terminal">
#ip netns exec qrouter-ca972d3b-788e-4e16-8552-dd335575c5c0 ping 10.0.0.2
</pre>

</li>

<li>
<p>To do ssh into VM, do the follwing:</p>

<pre class="terminal">
#ip netns exec qrouter-36f5ccde-1876-4554-be59-032af24c419c ssh 10.0.0.2 -l cirros
    userid: cirros
    password: cubswin:)
</pre>

</li>

<li>
<p>To get all the ports listed:</p>

<pre class="terminal">
#quantum port-list
</pre>

</li>

<li>
<p>Getting the VM Port-id for assigning Floating IP:</p>

<pre class="terminal">
#quantum port-list -c id -c fixed_ips -c device_owner 
</pre>

<table>
<tr bgcolor="#bad2e9">
<th>id</th>
<th>fixed-ips </th>
<th>device-owner</th>
</tr>
<tr>
<td>848d2e3a-ab3f-4e92-ad6d-269d793dde54</td>
<td>{"subnet_id": "6bfd8ae6-a2b9-4259-b813-4581a15c8d0f", "ip_address": "10.0.0.2"}</td>
<td> compute:None</td>
</tr>
<tr>
<td>9e069dc1-4af7-4c5f-a527-3c53d64affc6</td>
<td>{"subnet_id": "55cbc646-6271-4148-b426-a7e90619e28d", "ip_address": "10.42.0.2"}</td>
<td>network:router_gateway </td>
</tr>
<tr>
<td>b36a8a56-84db-4b2e-ab0e-2adf2cef88a4</td>
<td>{"subnet_id": "6bfd8ae6-a2b9-4259-b813-4581a15c8d0f", "ip_address": "10.0.0.3"}</td>
<td>network:dhcp </td>
</tr>
<tr>
<td>e40639e1-5986-4787-9f6c-452f608b24ff</td>
<td> {"subnet_id": "6bfd8ae6-a2b9-4259-b813-4581a15c8d0f", "ip_address": "10.0.0.1"}</td>
<td>network:router_interface </td>
</tr>
</table>
<p>The compute port-id is 848d2e3a-ab3f-4e92-ad6d-269d793dde54 , which has IP 10.0.0.2 in the private subnet id.</p>

<p>We will use this port id to create a floating IP in the Public network.</p>
<pre class="terminal">
848d2e3a-ab3f-4e92-ad6d-269d793dde54
</pre>

</li>

<li>
<p>Alternate way to get the VM port ID:</p>

<pre class="terminal">
#nova list
</pre>

</li>




<style>
table,td,th
{
border:1px solid black;
}
</style>
<table width="600">
<tr bgcolor="#bad2e9">
<th>ID</th>
<th>Name</th>
<th>Status</th>
<th>Networks</th>
</tr>
<tr>
<td>2b23d81b-b6db-4dfc-9440-ddf2ed4ef144</td>
<td>firstinstance</td>
<td>ACTIVE</td>
<td> private-net=10.0.0.2</td>
</tr>
</table>
<li>
<p>Now use the Device id to get the VM port ID:</p>

<pre class="terminal">
#quantum port-list -- --device_id  2b23d81b-b6db-4dfc-9440-ddf2ed4ef144
</pre>

</li>




<style>
table,td,th
{
border:1px solid black;
}
</style>
<table width="800">
<tr bgcolor="#bad2e9">
<th>ID</th>
<th>Name</th>
<th>mac_address</th>
<th>fixed_ips</th>
</tr>
<tr>
<td>848d2e3a-ab3f-4e92-ad6d-269d793dde54</td>
<td></td>
<td>fa:16:3e:79:9e:c4</td>
<td>{"subnet_id": "6bfd8ae6-a2b9-4259-b813-4581a15c8d0f", "ip_address": "10.0.0.2"}</td>
</tr>
</table>
<p>You can see that we got the VM PORT ID,  848d2e3a-ab3f-4e92-ad6d-269d793dde54 which is same as in the previous case.</p>

<li>
<p>Now create a floating IP for the VM with port_id as given above:</p>

<pre class="terminal">
#quantum floatingip-list
#should return empty list, since we have not created any ip
</pre>

</li>

<li>
<p>You can see subnet information with the following command:</p>

<pre class="terminal">
#quantum subnet-list
</pre>

</li>


<style>
table,td,th
{
border:1px solid black;
}
</style>
<table>
<tr bgcolor="#bad2e9">
<th>ID</th>
<th>Name</th>
<th>cidr</th>
<th>allocation_pools</th>
</tr>
<tr>
<td>55cbc646-6271-4148-b426-a7e90619e28d</td>
<td>public-net-subnet01</td>
<td>10.42.0.0/24</td>
<td>{"start": "10.42.0.2", "end": "10.42.0.254"}</td>
</tr>
<tr>
<td> 6bfd8ae6-a2b9-4259-b813-4581a15c8d0f</td>
<td> private-subnet </td>
<td>10.0.0.0/24</td>
<td> {"start": "10.0.0.2", "end": "10.0.0.254"}</td>
</tr>
</table>
<li>
<p>Command to see specific subnet information:</p>

<pre class="terminal">
#quantum subnet-show  55cbc646-6271-4148-b426-a7e90619e28d
</pre>

</li>

<li>
<p>Since we have not created allocation pool initially we need to update the subnet with an allocation pool:</p>
<p>But the below command is not working.</p>

<pre class="terminal">
#quantum subnet-update 55cbc646-6271-4148-b426-a7e90619e28d --allocation-pool start=10.42.0.75,end=10.42.0.254 public-net 10.42.0.0/24
</pre>

</li>

<li>
<p>Hence we will create floating point which is already there.</p>

<pre class="terminal">
#quantum floatingip-list
#quantum floatingip-create --port-id 848d2e3a-ab3f-4e92-ad6d-269d793dde54 public-net

   
    Created a new floatingip:
    +---------------------+--------------------------------------+
    | Field               | Value                                |
    +---------------------+--------------------------------------+
    | fixed_ip_address    | 10.0.0.2                             |
    | floating_ip_address | 10.42.0.3                            |
    | floating_network_id | 91cc9a4d-5c5f-4da5-8baf-7fe32f270cdb |
    | id                  | dad364f9-21eb-47c9-a29c-0489536b47eb |
    | port_id             | 848d2e3a-ab3f-4e92-ad6d-269d793dde54 |
    | router_id           | 36f5ccde-1876-4554-be59-032af24c419c |
    | tenant_id           | 6973efb023c748d6b8a4fff747faad92     |
    +---------------------+--------------------------------------+
</pre>

</li>

<li>
<p>ssh the virtual machine:</p>

<pre class="terminal">
#ssh 10.42.0.3 -l cirros 
    The authenticity of host '10.42.0.3 (10.42.0.3)' can't be established.
    RSA key fingerprint is da:f6:87:1a:3f:b6:e9:a4:92:8b:ca:a8:b8:d5:28:0d.
    Are you sure you want to continue connecting (yes/no)? yes
    Warning: Permanently added '10.42.0.3' (RSA) to the list of known hosts.
    #cirros@10.42.0.3's password: 
    cubswin:)
    $ 
</pre>

</li>

<li>
<p>Portforwarding:</p>

Enable Portforwarding on Gateway:

<pre class="terminal">
#eth1 src = TCP/IP port=25920   dest ip=10.42.0.3  dest port =  22 
</pre>

</li>

<li>
<p>Logging in to the VM from external network:</p>

<pre class="terminal">
#ssh 183.83.27.73 -p 25920 -l cirros
    The authenticity of host '10.42.0.3 (10.42.0.3)' can't be established.
    RSA key fingerprint is da:f6:87:1a:3f:b6:e9:a4:92:8b:ca:a8:b8:d5:28:0d.
    Are you sure you want to continue connecting (yes/no)? yes
    Warning: Permanently added '10.42.0.3' (RSA) to the list of known hosts.
    #cirros@10.42.0.3's password:
    cubswin:)
    $ $ 
    $ 
    $ pwd
    /home/cirros
    $ ls -al
    total 5
    drwxr-xr-x    2 cirros   cirros        1024 Jun 21 06:31 .
    drwxrwxr-x    4 root     root          1024 Oct 20  2011 ..
    -rw-------    1 cirros   cirros         119 Jun 21 07:22 .ash_history
    -rwxr-xr-x    1 cirros   cirros          43 Oct 20  2011 .profile
    -rwxr-xr-x    1 cirros   cirros          66 Oct 20  2011 .shrc
    #df -k
    Filesystem           1K-blocks      Used Available Use% Mounted on
    /dev                    248936         0    248936   0% /dev
    /dev/vda1                23797     13201      9368  58% /
    tmpfs                   252056         0    252056   0% /dev/shm
</pre>
</li>
</ul>
<a name="cinder-volume"><h2 id="cinder-volume">Create a Cinder Volume</h2></a>
<p>Cinder provides an infrastructure for managing volumes in OpenStack. Cinder services gives extra block level storage to your OpenStack Compute instances.</p>

<p>The following are the steps for the exercise:</p>

<ul>
<li>Create a Volume Type</li>
  <li>Create Volume</li>
  <li>Attach Volume to VM</li>
  <li>List Volumes</li>
  <li>Test the Volume</li>
  <li>FAQ's</li>
</ul><h3 id="create-a-volume-type">Create a Volume Type</h3>

<h6 id="using-cli">Using Cli</h6>

<pre class="terminal">
root@ubuntu:~# nova volume-type-create Backup
+--------------------------------------+--------+
| ID                                   | Name   |
+--------------------------------------+--------+
| 81fb6a1a-d927-40b0-a6ad-54a4241b9b3a | Backup |
+--------------------------------------+--------+
</pre>

<h6 id="using-dashboard">Using Dashboard</h6>

<p>Select <code>Admin</code>–&gt;<code>volumes</code>–&gt;<code>Create Volume Type</code></p>

<p><img src="./images/create_volume_type.png" alt="import"></p>

<h3 id="create-a-volume">Create a Volume</h3>

<p>Create a 1 GB test volume</p>

<h6 id="using-cli-1">Using Cli</h6>

<pre class="terminal">
root@ubuntu:~# cinder create --display_name volume1 1 --volume_type Backup
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|     attachments     |                  []                  |
|  availability_zone  |                 nova                 |
|       bootable      |                false                 |
|      created_at     |      2013-09-04T13:07:41.306051      |
| display_description |                 None                 |
|     display_name    |                volume1               |
|          id         | 4148fb29-4f38-4d29-9503-ed3f9ca49cf6 |
|       metadata      |                  {}                  |
|         size        |                  1                   |
|     snapshot_id     |                 None                 |
|     source_volid    |                 None                 |
|        status       |               creating               |
|     volume_type     |                Backup                |
+---------------------+--------------------------------------+
</pre>

<h6 id="using-dashboard-1">Using Dashboard</h6>

<p>Select <code>Project</code>–&gt;<code>volumes</code>–&gt;<code>Create Volume</code></p>

<p><img src="./images/create_volume.png" alt="import"></p>

<h3 id="attach-volume-to-vm">Attach Volume to VM</h3>

<h6 id="using-cli-2">Using Cli</h6>

<pre class="terminal">
root@ubuntu:~# nova volume-attach vm2 310dadc8-ed30-43f1-a7fd-b474fb762cdd /dev/vdd
+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdd                             |
| serverId | 73cea228-3c5d-4846-9d71-6ba14a5da374 |
| id       | 310dadc8-ed30-43f1-a7fd-b474fb762cdd |
| volumeId | 310dadc8-ed30-43f1-a7fd-b474fb762cdd |
+----------+--------------------------------------+
</pre>

<h6 id="using-dashboard-2">Using Dashboard</h6>

<p>Select <code>volumes</code>–&gt;<code>Edit Attachments</code></p>

<p><img src="./images/manage_volume_attachments.png" alt="import"></p>

<h3 id="list-volumes">List Volumes</h3>

<h6 id="using-cli-3">Using Cli</h6>

<pre class="terminal">
root@ubuntu:~# cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 20767a0b-95e5-426e-9d22-cfe614e561ca |   in-use  |    volume    |  1   |     SSD     |  false   | 73cea228-3c5d-4846-9d71-6ba14a5da374 |
| 310dadc8-ed30-43f1-a7fd-b474fb762cdd | available |   volume1    |  1   |     SSD     |  false   |                                      |
| 4148fb29-4f38-4d29-9503-ed3f9ca49cf6 | available |    test2     |  1   |    Backup   |  false   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
</pre>

<h6 id="using-dashboard-3">Using Dashboard</h6>

<p><img src="./images/volume_list.png" alt="import"></p>

<h3 id="test-the-volume">Test the Volume</h3>

<p>After attaching the volume, ssh in to the VM and format the volume.</p>

<h5 id="ssh-in-to-vm-using-floating-ip-address">1. SSH in to VM using floating IP address</h5>

<pre class="terminal">
#ssh cirros@192.168.0.76
#passwd: cubswin:)

Enter into sudo mode:

#sudo su

check the partiton table:

#fdisk -l
</pre>

<h5 id="create-a-valid-partition-for-the-volume-devvdd">2. Create a valid partition for the volume ‘/dev/vdd'</h5>

<pre class="terminal">
$fdisk /dev/vdd
Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel
Building a new DOS disklabel with disk identifier 0x69e08462.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won't be recoverable.

Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4, default 1): 
Using default value 1
First sector (2048-2097151, default 2048): 
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-2097151, default 2097151): 
Using default value 2097151

Command (m for help): p

Disk /dev/vdd: 1073 MB, 1073741824 bytes
16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x69e08462

   Device Boot      Start         End      Blocks   Id  System
/dev/vdd1            2048     2097151     1047552   83  Linux

Command (m for help): q
</pre>

<h5 id="create-file-system-for-the-volume">3. Create file system for the Volume</h5>

<pre class="terminal">
$mkfs.ext3 /dev/vdd1
</pre>

<h5 id="mount-the-partiton">4. Mount the Partiton</h5>

<pre class="terminal">
$ mkdir -p /mnt/data
$ mount /dev/vdd1 /mnt/data/
$ df -h
Filesystem                Size      Used Available Use% Mounted on
/dev                    243.1M         0    243.1M   0% /dev
/dev/vda1                23.2M     12.9M      9.1M  59% /
tmpfs                   246.1M         0    246.1M   0% /dev/shm
tmpfs                   200.0K     20.0K    180.0K  10% /run
/dev/vdd               1007.9M     33.3M    923.4M   3% /mnt/data

$ cd /mnt/data/

$ cat &gt; file1
HI THIS IS A TEST FILE
$ cat file1
HI THIS IS A TEST FILE
</pre>

<p>Now, we can also detach <code>/dev/vdd</code> volume and attach it to any other instance and access the data. </p>

<h3 id="faqs">FAQ's</h3>

<h5 id="if-volume-creation-fails">1. If volume creation fails</h5>
<p>Execute the below commands to fix the issues</p>

<p>If <code>tgt</code> service is running, Stop it by using the command:</p>

<pre class="terminal">
#service tgt stop
#service iscsitarget restart 
#service open-iscsi restart
</pre>

<p>If you find an error similar to this in <code>cinder-volume.log</code> file:</p>

<pre class="terminal">
ProcessExecutionError: Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf ietadm --op new --tid=1 --params Name=iqn.2010-10.org.openstack:volume-19bb7c48-45f5-4034-b30b-c52863d34214
Exit code: 239
Stdout: ''
Stderr: 'File exists.\n'
</pre>

<p>Then execute the below command:</p>

<pre class="terminal">
#ietadm --op delete --tid=1
</pre>

<p><code>Note: For tid check /var/log/cinder/cinder-volume.log file</code></p>

<h5 id="check-if-cinder-is-running-on-the-ports-usually-iscsi-runs-on-these-port">2. Check if cinder is running on the ports; Usually iscsi runs on these port</h5>

<pre class="terminal">
#ss -tuplen | grep 3260

expected output:
tcp    LISTEN     0      32                    :::3260                 :::*      users:(("ietd",4800,8)) ino:9045748 sk:ffff880413fb4380
tcp    LISTEN     0      32                     *:3260                  *:*      users:(("ietd",4800,7)) ino:9045747 sk:ffff880304dea1c0
</pre>

<p>Note you can also use the following command:</p>

<pre class="terminal">
#lsof -i -a | grep 3260

Expected output:
ietd       4800         root    7u  IPv4  9045747      0t0  TCP *:3260 (LISTEN)
ietd       4800         root    8u  IPv6  9045748      0t0  TCP *:3260 (LISTEN)
iscsid    19618         root    9u  IPv4  9483329      0t0  TCP ubuntu.root:49065-&gt;ubuntu.root:3260 (ESTABLISHED)
</pre>

<h5 id="i-can-create-volumes-but-cant-attach-to-an-existing-instance">3. I can create volumes but can't attach to an existing instance.</h5>

<p>The work around that issue was:</p>

<ul>
<li>In Horizon, delete the volume that was created, because this one used tgt to create</li>
  <li>Stop tgt service –&gt; tgt stop</li>
  <li>Restart iscsi –&gt; service iscsitarget restart ; service open-iscsi restart</li>
  <li>In Horizon, create a new volume</li>
  <li>Then attach it to an instance</li>
</ul>

<a name="authors"><h2>About the Authors</h2></a>
<p><b>Rajdeep Dua</b> : Rajdeep Dua is part VMware team in India working on OpenStack and related projects. He is
involved in Academic and OpenSource outreach activities. Before joining VMware he lead Developer Relations team in Google India with focus
on Google Cloud and Android. Rajdeep his Master in Management from IIM Lucknow, India. He can be
reached at @rajdeepdua on Twitter</p>

<p><b>A ReddyRaja</b> : A ReddyRaja is Founder and CEO of Akrantha Software. His focus areas are provided consulting services on OpenStack and Big Data.
He worked with Pramati, HP and Tata Institute of Fundamental Research.</p>
</div>
</div>
</div>
</body>
</html>
